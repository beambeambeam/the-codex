from collections import Counter, defaultdict

import numpy as np
import umap
from bertopic import BERTopic
from hdbscan import HDBSCAN
from loguru import logger
from tqdm.auto import tqdm

from ....document.schemas import ChunkResponse, DocumentResponse
from ....document.service import DocumentService
from ...core import call_llm
from ..prompts import render_keyword_to_topic_extraction
from .schemas import ClusteringResult, DocumentDistribution, TopicCluster


class DocumentClusteringService:
    """Service for clustering document chunks based on their embeddings."""

    def __init__(
        self,
        document_service: DocumentService,
    ):
        self._DOCUMENT_SERVICE = document_service
        self._UMAP_MODEL = umap.UMAP(
            n_neighbors=15, metric="cosine", random_state=42, n_components=2
        )
        self._HDBSCAN_MODEL = HDBSCAN(
            min_cluster_size=3,
            max_cluster_size=10,
            cluster_selection_epsilon=0.4,
            prediction_data=True,
        )
        self.TOPIC_MODEL = BERTopic(
            umap_model=self._UMAP_MODEL,
            hdbscan_model=self._HDBSCAN_MODEL,
            calculate_probabilities=True,
            top_n_words=50,
        )

    def get_collection_chunk_dict(
        self,
        collection_id: str,
    ) -> tuple[dict[str, list[ChunkResponse]], dict[str, DocumentResponse]]:
        """Fetches chunk embeddings for a given collection and maps doc_id to document."""
        documents = self._DOCUMENT_SERVICE.get_collection_documents(
            collection_id=collection_id
        )
        documents = [DocumentResponse.model_validate(doc) for doc in documents]

        chunk_embeddings = defaultdict(list)
        doc_id_to_doc = {}

        for doc in documents:
            doc_id_to_doc[doc.id] = doc
            chunks = self._DOCUMENT_SERVICE.get_document_chunks(document_id=doc.id)
            chunks = [ChunkResponse.model_validate(chunk) for chunk in chunks]

            for chunk in chunks:
                if chunk.embedding is not None:
                    chunk_embeddings[doc.id].append(chunk)

        return chunk_embeddings, doc_id_to_doc

    def llm_generate_cluster_title(self, keywords: list[str]) -> str:
        """
        Generates a descriptive title for a cluster of topics using an LLM.

        Parameters:
        - keywords: List of keywords representing the cluster.

        Returns:
        - A string title generated by the LLM.
        """
        keywords = ", ".join(keywords)
        prompt = render_keyword_to_topic_extraction(keywords=keywords)
        print(f"Generating cluster title with keywords: {keywords[:50]}")
        return call_llm(prompt)

    def _get_topic_keyword_map(
        self, topic_info_df, top_words: int
    ) -> dict[int, list[str]]:
        """
        (Helper) Creates a map from topic ID to its top keywords.
        This is more efficient than rebuilding the map on each call.
        """
        return {
            row.Topic: row.Representation[:top_words]
            for _, row in topic_info_df.iterrows()
        }

    def _get_keywords_for_topics(
        self, topic_keyword_map: dict[int, list[str]], topic_ids: list[int]
    ) -> list[str]:
        """(Helper) Fetch keywords for a list of topic IDs from the pre-built map."""
        keywords = []
        for topic_id in topic_ids:
            keywords.extend(topic_keyword_map.get(topic_id, []))
        return keywords

    def cluster_documents(
        self,
        collection_id: str,
        cluster_title_top_n_topics: int = 5,
        cluster_title_top_n_words: int = 10,
    ) -> ClusteringResult:
        """
        Clusters document chunks in a collection and generates descriptive topic titles.
        """
        chunk_embeddings, doc_id_to_doc = self.get_collection_chunk_dict(collection_id)
        if not chunk_embeddings:
            return ClusteringResult(topics={}, documents={})

        logger.info(
            f"Starting clustering for collection {collection_id} with {len(chunk_embeddings)} documents."
        )

        chunk_texts, embeddings, doc_ids = [], [], []
        for doc_id, chunks in chunk_embeddings.items():
            for chunk in chunks:
                chunk_texts.append(chunk.chunk_text)
                embeddings.append(chunk.embedding)
                doc_ids.append(doc_id)

        # 1. Perform Topic Modeling
        embeddings = np.array(embeddings)

        topics, _ = self.TOPIC_MODEL.fit_transform(chunk_texts, embeddings=embeddings)
        logger.info(
            f"BERTopic clustering completed for collection {collection_id}. Found {len(set(topics))} topics."
        )

        topic_info = self.TOPIC_MODEL.get_topic_info()

        # 2. Pre-build maps for efficiency
        topic_id_to_name = {row.Topic: row.Name for _, row in topic_info.iterrows()}
        topic_keyword_map = self._get_topic_keyword_map(
            topic_info, top_words=cluster_title_top_n_words
        )

        # 3. Calculate per-document topic distributions
        doc_chunk_topics = defaultdict(list)
        for doc_id, topic in zip(doc_ids, topics):
            doc_chunk_topics[doc_id].append(topic)

        doc_distributions = {
            doc_id: Counter(topic_list)
            for doc_id, topic_list in doc_chunk_topics.items()
        }
        doc_primary_topic = {
            doc_id: counter.most_common(1)[0][0]
            for doc_id, counter in doc_distributions.items()
        }

        # 4. Aggregate topic counts per cluster (defined by the primary topic)
        cluster_topic_counters = defaultdict(Counter)
        for doc_id, counter in doc_distributions.items():
            primary_topic_id = doc_primary_topic[doc_id]
            cluster_topic_counters[primary_topic_id].update(counter)

        logger.info(
            f"Aggregated topic counts for {len(cluster_topic_counters)} clusters."
        )

        # 5. Generate a descriptive title for each cluster using an LLM
        cluster_titles = {}
        for primary_topic_id, aggregated_counter in tqdm(
            list(cluster_topic_counters.items()), desc="Generating cluster titles"
        ):
            contributing_topic_ids = [
                topic_id
                for topic_id, _ in aggregated_counter.most_common(
                    cluster_title_top_n_topics
                )
            ]
            keywords = self._get_keywords_for_topics(
                topic_keyword_map, contributing_topic_ids
            )

            if keywords:
                cluster_titles[primary_topic_id] = self.llm_generate_cluster_title(
                    keywords
                )
                print(
                    f"- Generated title for topic {primary_topic_id}: **{cluster_titles[primary_topic_id]}**"
                )

        # 6. Structure the final output
        topics_result: dict[str, TopicCluster] = {}
        documents_result: dict[str, DocumentDistribution] = {}

        for doc_id, topic_counter in doc_distributions.items():
            primary_topic_id = doc_primary_topic[doc_id]

            cluster_label = cluster_titles.get(
                primary_topic_id,
                topic_id_to_name.get(primary_topic_id, f"Topic {primary_topic_id}"),
            )

            label_distribution = {
                topic_id_to_name.get(topic_id, f"Topic {topic_id}"): count
                for topic_id, count in topic_counter.items()
            }

            documents_result[doc_id] = DocumentDistribution(
                top_topic=cluster_label,
                distribution=label_distribution,
            )

            # Add document to its corresponding cluster
            if cluster_label not in topics_result:
                topics_result[cluster_label] = TopicCluster(
                    topic_id=primary_topic_id, documents=[]
                )
            topics_result[cluster_label].documents.append(doc_id_to_doc[doc_id])

        logger.info(
            f"Clustering completed for collection {collection_id}. Found {len(topics_result)} clusters."
        )
        return ClusteringResult(
            topics=topics_result,
            documents=documents_result,
        )
