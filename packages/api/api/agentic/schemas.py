from enum import Enum

from pydantic import BaseModel, Field

from api.chat.schemas import (
    CollectionChatHistoryBase,
    CollectionChatResponse,
)
from api.document.schemas import (
    ChunkSearchResponse,
)
from api.models.user import User

from .pocketflow_custom import ShareStoreBase


class NodeStatus(str, Enum):
    DEFAULT = "default"  # Default status for nodes
    RETRY = "retry"  # Status to indicate a retry is needed
    ERROR = "error"  # Status to indicate an error occurred


class INTENT(str, Enum):
    DOCUMENT_QA = "document_qa"  # Intent for document question answering
    GENERIC_QA = "generic_qa"  # Intent for generic question answering
    SUMMARIZATION = "summarization"  # Intent for summarization tasks


class UserIntent(BaseModel):
    """Schema for user intent in the RAG system."""

    intent: INTENT = Field(
        ...,
        description="""
        The intent of the user's query, e.g., 'document_qa', 'generic_qa', 'summarization'
        - document_qa: For questions related to specific documents

        Example:
        Q: "What does the document say about climate change?"
        A: "document_qa"

        Q: "What are the key findings in the report?"
        A: "document_qa"

        - generic_qa: For general questions not tied to specific documents

        Example:
        Q: "What is the capital of France?"
        A: "generic_qa"

        Q: "How does photosynthesis work?"
        A: "generic_qa"
        
        - summarization: For requests to summarize information

        Example:
        Q: "Summarize the key points of this article."
        A: "summarization"
 
        Q: "Can you provide overview of this document?"
        A: "summarization"

        if the intent is not recognized, it should be classified as 'generic_qa'
    """,
    )
    confidence: float = Field(
        1.0, description="Confidence score of the identified intent (0.0 to 1.0)"
    )


class ChatMessage(CollectionChatHistoryBase):
    """Schema for chat messages in the RAG system."""

    collection_chat_id: str = Field(None, description="Collection Chat ID")
    retrieved_contexts: list[ChunkSearchResponse] = Field(
        default_factory=list,
        description="List of context references retrieved for the chat message",
    )

    class Config:
        from_attributes = True


class ChatHistory(BaseModel):
    """Schema for a list of chat messages."""

    messages: list[ChatMessage] = Field(
        default_factory=list, description="List of chat messages in the conversation"
    )

    def to_list(self) -> list[dict]:
        """
        Convert the ChatMessageList to a list of dictionaries.
        This is useful for serialization or further processing.
        """
        return [message.model_dump() for message in self.messages]


# NodeTypes = Literal["EmbedChunksNode", "StoreInPgvectorNode", "SearchPgvectorNode", "GenerateResponseNode"]


class AgentResponse(BaseModel):
    """Schema for the response from the RAG agent."""

    # answer: ChatMessage = Field(
    #     ..., description="The answer generated by the RAG agent"
    # )
    chat_history: ChatHistory = Field(
        default_factory=ChatHistory,
        description="Conversation history including user questions and assistant responses",
    )
    retrieved_contexts: list[ChunkSearchResponse] = Field(
        default_factory=list,
        description="List of retrieved document chunks based on the query embedding",
    )


class SharedStore(ShareStoreBase):
    # Offline Indexing Flow
    input_file_paths: list[str] = Field(
        default_factory=list, description="List of file paths to be indexed"
    )
    parsed_chunks_for_embedding: list[ChunkSearchResponse] = Field(
        default_factory=list,
        description="List of parsed document chunks ready for embedding",
    )
    processed_chunks: list[ChunkSearchResponse] = Field(
        default_factory=list,
        description="List of processed document chunks with embeddings",
    )

    # Online Querying Flow
    user_question: str = Field(
        None, description="User's question or query for the RAG system"
    )
    query_embedding: list[float] = Field(
        default_factory=list, description="Embedding vector of the user's question"
    )
    retrieved_contexts: list[ChunkSearchResponse] = Field(
        default_factory=list,
        description="List of retrieved document chunks based on the query embedding",
    )
    llm_answer: ChatMessage = Field(
        None,
        description="Final answer generated by the LLM based on the retrieved contexts",
    )

    # General / Conversational
    chat_history: ChatHistory = Field(
        default_factory=ChatHistory,
        description="Conversation history including user questions and assistant responses",
    )
    system_instructions: str = Field(
        None,
        description="System instructions or context to guide the LLM's responses",
    )

    # Saving Data
    new_chat_history: ChatHistory = Field(
        default_factory=ChatHistory,
        description="New chat history to be appended to the existing conversation",
    )

    # References
    document_references_id: list[str] = Field(
        default_factory=list,
        description="ID of the document being processed or queried",
    )

    # Node Status
    current_node: str = Field(
        None, description="Name of the current node being executed in the flow"
    )
    user_intent: UserIntent = Field(
        None,
        description="User's intent for the current query, e.g., 'document_qa', 'generic_qa'",
    )

    # Session
    chat_session: CollectionChatResponse = Field(
        None, description="Chat collection for storing conversation history"
    )
    current_user: User = Field(
        None, description="ID of the current user interacting with the RAG system"
    )

    class Config:
        arbitrary_types_allowed = True
